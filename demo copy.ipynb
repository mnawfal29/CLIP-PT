{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor\n",
    "from src.datasets import CIFAR100FSCIL, CUB200FSCIL, MiniImageNetFSCIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset along with Image and Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muhdn\\miniconda3\\envs\\fyp\\Lib\\site-packages\\transformers\\models\\clip\\processing_clip.py:149: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "img_preprocess = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\").feature_extractor\n",
    "text_preprocess = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\").tokenizer\n",
    "dataset = CIFAR100FSCIL(transform=img_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§· Model Parameters\n",
    "\n",
    "Model initialized as:  \n",
    "`CLIPForPromptTuning(L_g=4, L_s=4, D_g=10, D_s=2)`\n",
    "\n",
    "- `L_g = 4`: Number of **General Prompts (G-prompts)** per modality (vision & language).\n",
    "- `L_s = 4`: Number of **Shared Prompts (S-prompts)** across modalities.\n",
    "- `D_g = 10`: Number of layers/depth for which **G-prompts** are inserted.\n",
    "- `D_s = 2`: Number of layers/depth for which **S-prompts** are inserted.\n",
    "- Total layers in CLIP text and image encoder -> 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.clip_models import CLIPForPromptTuning\n",
    "model = CLIPForPromptTuning(L_g=4, L_s=4, D_g=10, D_s=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 100\n",
      "Model Classes: ['apple', 'aquarium fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak tree', 'orange', 'orchid', 'otter', 'palm tree', 'pear', 'pickup truck', 'pine tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow tree', 'wolf', 'woman', 'worm']\n"
     ]
    }
   ],
   "source": [
    "text_label_mapping = dataset.text_label_mapping.values()\n",
    "prompt_labels = ['[]'.replace('[]', i) for i in text_label_mapping]\n",
    "print(f\"Number of classes: {len(prompt_labels)}\")\n",
    "print(f\"Model Classes: {prompt_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_text_tokens = text_preprocess(prompt_labels, padding=True, return_tensors=\"pt\")\n",
    "text_tokens = out_text_tokens[\"input_ids\"].to(device)\n",
    "attn_mask = out_text_tokens[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset length: 6000\n"
     ]
    }
   ],
   "source": [
    "session_no = 0 # Base Session\n",
    "test_dataset = dataset.eval_stream[session_no]._datasets[0]\n",
    "test_length = len(test_dataset)\n",
    "print(f\"Test dataset length: {test_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "rand = random.randint(0, test_length - 1)\n",
    "img, x, y = test_dataset[rand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: bear\n"
     ]
    }
   ],
   "source": [
    "print(f\"True label: {prompt_labels[y]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21.9442, 21.1817, 22.2317, 25.6748, 24.9217, 19.4653, 22.0614, 19.9956,\n",
       "         19.9834, 20.8015, 21.1082, 21.2212, 22.0211, 19.2919, 21.6196, 25.7606,\n",
       "         22.1121, 19.9578, 20.5058, 25.8859, 20.7491, 24.8579, 21.5507, 21.9781,\n",
       "         18.8758, 20.9095, 20.5628, 21.2551, 20.1599, 22.5594, 22.4197, 23.4722,\n",
       "         22.3390, 23.7674, 23.0566, 20.6650, 23.0290, 19.6536, 24.5385, 21.4802,\n",
       "         19.5188, 22.9245, 21.9093, 24.3556, 19.8583, 20.8884, 22.3249, 21.9323,\n",
       "         19.5393, 22.4188, 22.5605, 21.9058, 20.1038, 21.6178, 22.7976, 23.1640,\n",
       "         20.0658, 20.4957, 20.9667, 21.8960, 21.7744, 20.0843, 21.0496, 24.9927,\n",
       "         23.3907, 22.7302, 22.9145, 21.1126, 21.8598, 21.1380, 22.1159, 22.6109,\n",
       "         22.3848, 21.5915, 22.7176, 23.7071, 21.0948, 20.7128, 20.4113, 19.8888,\n",
       "         24.2704, 20.1246, 21.4347, 22.8881, 21.9425, 20.9563, 22.3376, 23.3204,\n",
       "         23.4490, 21.8377, 21.2983, 22.6431, 21.7847, 21.0763, 19.7935, 22.6815,\n",
       "         22.2401, 23.9674, 21.1579, 19.7198]], device='cuda:0',\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = torch.tensor(x, device=device).unsqueeze(0)\n",
    "logits, text_out = model(x_new, text_tokens, attn_mask)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100]), torch.Size([100, 512]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, text_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: bear\n",
      "Predicted: cattle\n"
     ]
    }
   ],
   "source": [
    "output_idx = torch.argmax(logits, dim=1).item()\n",
    "print(f\"True: {prompt_labels[y]}\")\n",
    "print(f\"Predicted: {prompt_labels[output_idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
